{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXp6_2ZcDpSS",
        "outputId": "dc8e0a74-bc2d-44d5-9bd9-56f3a09bb5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"\")\n",
        "project = rf.workspace(\"project-9vrlu\").project(\"accident-2-jns6f\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31NIsq3SDspA",
        "outputId": "d81b8d6d-5e10-473d-8a4d-2cdde9acd655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.45-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.8.30)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.53.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Downloading roboflow-1.1.45-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, python-dotenv, idna, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 python-dotenv-1.0.1 requests-toolbelt-1.0.0 roboflow-1.1.45\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Dependency ultralytics==8.0.196 is required but found version=8.3.2, to fix: `pip install ultralytics==8.0.196`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in accident-2-1 to yolov8:: 100%|██████████| 606610/606610 [00:10<00:00, 57942.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to accident-2-1 in yolov8:: 100%|██████████| 16454/16454 [00:03<00:00, 5054.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "results = model.train(data=\"/content/accident-2-1/data.yaml\", epochs=10, imgsz=640)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tR8nTxpcD97f",
        "outputId": "d29a48a5-73c9-4143-cbef-0005a01de44d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 144MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.2 🚀 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/accident-2-1/data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 23.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=6\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    431842  ultralytics.nn.modules.head.Detect           [6, [64, 128, 256]]           \n",
            "Model summary: 249 layers, 2,691,378 parameters, 2,691,362 gradients, 6.9 GFLOPs\n",
            "\n",
            "Transferred 313/391 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLO11n...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 99.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ yolo11n.pt appears to require 'dill', which is not in Ultralytics requirements.\n",
            "AutoInstall will run now for 'dill' but this feature will be removed in the future.\n",
            "Recommend fixes are to train a new model using the latest 'ultralytics' package or to run a command with an official Ultralytics model, i.e. 'yolo predict model=yolov8n.pt'\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['dill'] not found, attempting AutoUpdate...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dill\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.4/119.4 kB 8.8 MB/s eta 0:00:00\n",
            "Installing collected packages: dill\n",
            "Successfully installed dill-0.3.9\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 2.3s, installed 1 package: ['dill']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/accident-2-1/train/labels... 6831 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6831/6831 [00:04<00:00, 1633.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/accident-2-1/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of Albumentations is available: 1.4.17 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/accident-2-1/valid/labels... 959 images, 0 backgrounds, 0 corrupt: 100%|██████████| 959/959 [00:01<00:00, 849.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/accident-2-1/valid/labels.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 63 weight(decay=0.0), 70 weight(decay=0.0005), 69 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10      2.32G      1.083      3.591      1.464         16        640:  59%|█████▉    | 253/427 [01:22<00:57,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b5dc2f858676>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov8n.pt\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load a pretrained model (recommended for training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/accident-2-1/data.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[0;31m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mni\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_opt_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m                     \u001b[0mlast_opt_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mni\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# unscale gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clip gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m                             )\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    225\u001b[0m             )\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mgrad_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfound_inf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_group_tensors_by_device_and_dtype\u001b[0;34m(tensorlistlist, with_indices)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensorlistlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorlistlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_group_tensors_by_device_and_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorlistlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_indices\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[return-value, arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_patch_step_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_foreach_utils.py\u001b[0m in \u001b[0;36m_group_tensors_by_device_and_dtype\u001b[0;34m(tensorlistlist, with_indices)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mwith_indices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m ) -> Dict[Tuple[torch.device, torch.dtype], Tuple[TensorListList, Indices]]:\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_tensors_by_device_and_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorlistlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_device_has_foreach_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the custom YOLO model\n",
        "model = YOLO(\"/content/best.pt\")\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(\"/content/WhatsApp Video 2024-10-01 at 20.16.28_7047498d.mp4\")\n",
        "assert cap.isOpened(), \"Error reading video file\"\n",
        "\n",
        "# Get video properties\n",
        "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Video writer to save the output\n",
        "video_writer = cv2.VideoWriter(\"object_detection_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, im0 = cap.read()\n",
        "    if not success:\n",
        "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
        "        break\n",
        "\n",
        "    # Perform object detection\n",
        "    results = model.track(im0, persist=True, show=False)\n",
        "\n",
        "    # Filter detections by confidence\n",
        "    # Access the boxes object within the results object and filter\n",
        "    results[0].boxes = results[0].boxes[results[0].boxes.conf >= 0.6]\n",
        "\n",
        "    # Plot the results\n",
        "    im0_filtered = results[0].plot()\n",
        "\n",
        "    # Write the frame with detections to the output video\n",
        "    video_writer.write(im0_filtered)\n",
        "\n",
        "# Release the video capture and writer objects\n",
        "cap.release()\n",
        "video_writer.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "333EX9cmEI6F",
        "outputId": "1291165b-aaba-4a5f-d3af-b3b8f479fdbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 320x640 1 moderate, 37.2ms\n",
            "Speed: 1.8ms preprocess, 37.2ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 2.8ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 7.0ms\n",
            "Speed: 2.9ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.8ms\n",
            "Speed: 2.5ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 7.1ms\n",
            "Speed: 2.3ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.8ms\n",
            "Speed: 2.1ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.7ms\n",
            "Speed: 2.0ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.9ms\n",
            "Speed: 1.8ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.9ms\n",
            "Speed: 2.9ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 2.2ms preprocess, 6.6ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 20.2ms\n",
            "Speed: 5.0ms preprocess, 20.2ms inference, 3.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 10.9ms\n",
            "Speed: 2.7ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.7ms\n",
            "Speed: 1.5ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 2.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.8ms\n",
            "Speed: 2.6ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.5ms\n",
            "Speed: 2.8ms preprocess, 6.5ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.8ms\n",
            "Speed: 2.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.8ms\n",
            "Speed: 2.1ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 1 moderate, 6.5ms\n",
            "Speed: 1.5ms preprocess, 6.5ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 1 moderate, 6.7ms\n",
            "Speed: 1.6ms preprocess, 6.7ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 1 moderate, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 1 moderate, 11.9ms\n",
            "Speed: 2.2ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 7.0ms\n",
            "Speed: 2.0ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.7ms\n",
            "Speed: 2.1ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.7ms\n",
            "Speed: 2.0ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.7ms\n",
            "Speed: 2.7ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 2.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.6ms\n",
            "Speed: 2.4ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.0ms\n",
            "Speed: 2.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 2.8ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.4ms\n",
            "Speed: 3.2ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.7ms\n",
            "Speed: 1.6ms preprocess, 6.7ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.7ms\n",
            "Speed: 3.0ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.7ms\n",
            "Speed: 3.4ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.3ms\n",
            "Speed: 2.1ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.8ms\n",
            "Speed: 2.1ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.7ms\n",
            "Speed: 2.3ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.3ms\n",
            "Speed: 2.3ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.7ms\n",
            "Speed: 2.6ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.4ms\n",
            "Speed: 3.3ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.0ms\n",
            "Speed: 2.7ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 2.6ms preprocess, 6.6ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.5ms\n",
            "Speed: 2.6ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.5ms\n",
            "Speed: 2.6ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 2.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.1ms\n",
            "Speed: 2.9ms preprocess, 9.1ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.8ms\n",
            "Speed: 2.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.7ms\n",
            "Speed: 2.9ms preprocess, 6.7ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 3.1ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.6ms\n",
            "Speed: 3.0ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.4ms\n",
            "Speed: 2.7ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.5ms\n",
            "Speed: 2.8ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.6ms\n",
            "Speed: 2.6ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.4ms\n",
            "Speed: 2.9ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.4ms\n",
            "Speed: 2.7ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.7ms\n",
            "Speed: 2.8ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.5ms\n",
            "Speed: 2.7ms preprocess, 6.5ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.9ms\n",
            "Speed: 2.9ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 9.1ms\n",
            "Speed: 2.7ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 9.1ms\n",
            "Speed: 2.2ms preprocess, 9.1ms inference, 0.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 8.6ms\n",
            "Speed: 1.5ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 8.9ms\n",
            "Speed: 3.5ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 8.9ms\n",
            "Speed: 3.3ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 8.7ms\n",
            "Speed: 2.2ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 8.6ms\n",
            "Speed: 2.0ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 10.2ms\n",
            "Speed: 2.8ms preprocess, 10.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.7ms\n",
            "Speed: 2.1ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.4ms\n",
            "Speed: 2.0ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 1.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 2.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 3.5ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.4ms\n",
            "Speed: 2.8ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 12.6ms\n",
            "Speed: 2.1ms preprocess, 12.6ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 2.0ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 7.0ms\n",
            "Speed: 1.9ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.7ms\n",
            "Speed: 2.1ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.7ms\n",
            "Speed: 2.9ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.0ms\n",
            "Speed: 2.0ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 3.0ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 3.0ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.0ms\n",
            "Speed: 2.9ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.8ms\n",
            "Speed: 3.8ms preprocess, 10.8ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 9.3ms\n",
            "Speed: 3.1ms preprocess, 9.3ms inference, 0.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 10.2ms\n",
            "Speed: 2.4ms preprocess, 10.2ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.4ms\n",
            "Speed: 2.7ms preprocess, 9.4ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 14.3ms\n",
            "Speed: 4.5ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 13.8ms\n",
            "Speed: 2.2ms preprocess, 13.8ms inference, 2.1ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 10.9ms\n",
            "Speed: 2.4ms preprocess, 10.9ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 14.1ms\n",
            "Speed: 3.3ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 12.6ms\n",
            "Speed: 2.5ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 11.7ms\n",
            "Speed: 2.4ms preprocess, 11.7ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 8.7ms\n",
            "Speed: 2.6ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.6ms\n",
            "Speed: 4.7ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 11.2ms\n",
            "Speed: 2.2ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 8.6ms\n",
            "Speed: 2.5ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.0ms\n",
            "Speed: 2.7ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 8.2ms\n",
            "Speed: 2.3ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 8.7ms\n",
            "Speed: 2.4ms preprocess, 8.7ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 8.0ms\n",
            "Speed: 2.3ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 8.4ms\n",
            "Speed: 2.6ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 12.0ms\n",
            "Speed: 2.4ms preprocess, 12.0ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 13.4ms\n",
            "Speed: 2.1ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 8.2ms\n",
            "Speed: 2.4ms preprocess, 8.2ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.7ms\n",
            "Speed: 2.3ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.6ms\n",
            "Speed: 2.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.8ms\n",
            "Speed: 2.2ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 8.0ms\n",
            "Speed: 2.1ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 11.0ms\n",
            "Speed: 3.1ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.7ms\n",
            "Speed: 2.2ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 11.6ms\n",
            "Speed: 2.8ms preprocess, 11.6ms inference, 3.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 13.0ms\n",
            "Speed: 2.6ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 11.9ms\n",
            "Speed: 2.2ms preprocess, 11.9ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 13.2ms\n",
            "Speed: 2.7ms preprocess, 13.2ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.2ms\n",
            "Speed: 2.6ms preprocess, 9.2ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.7ms\n",
            "Speed: 2.3ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 22.7ms\n",
            "Speed: 3.2ms preprocess, 22.7ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 17.7ms\n",
            "Speed: 3.8ms preprocess, 17.7ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 14.1ms\n",
            "Speed: 4.3ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 15.6ms\n",
            "Speed: 2.2ms preprocess, 15.6ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 12.0ms\n",
            "Speed: 3.5ms preprocess, 12.0ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 13.4ms\n",
            "Speed: 2.1ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 13.5ms\n",
            "Speed: 2.2ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 11.7ms\n",
            "Speed: 2.3ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.4ms\n",
            "Speed: 2.1ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.2ms\n",
            "Speed: 2.7ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.1ms\n",
            "Speed: 2.5ms preprocess, 9.1ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.6ms\n",
            "Speed: 2.1ms preprocess, 10.6ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 52.8ms\n",
            "Speed: 2.1ms preprocess, 52.8ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 12.2ms\n",
            "Speed: 2.3ms preprocess, 12.2ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 11.9ms\n",
            "Speed: 2.1ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 15.0ms\n",
            "Speed: 2.9ms preprocess, 15.0ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.8ms\n",
            "Speed: 2.1ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.3ms\n",
            "Speed: 2.6ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.5ms\n",
            "Speed: 2.2ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.4ms\n",
            "Speed: 2.6ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 11.6ms\n",
            "Speed: 2.0ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 43.8ms\n",
            "Speed: 4.1ms preprocess, 43.8ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 18.6ms\n",
            "Speed: 2.7ms preprocess, 18.6ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 12.3ms\n",
            "Speed: 2.1ms preprocess, 12.3ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 14.4ms\n",
            "Speed: 3.5ms preprocess, 14.4ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 7.6ms\n",
            "Speed: 2.5ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 9.5ms\n",
            "Speed: 2.9ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 9.0ms\n",
            "Speed: 2.4ms preprocess, 9.0ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 10.0ms\n",
            "Speed: 2.2ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 9.9ms\n",
            "Speed: 2.1ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.8ms\n",
            "Speed: 2.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "Video frame is empty or video processing has been successfully completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install twilio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMfiJmW_4UKq",
        "outputId": "0f8b9c0b-defb-4a76-a82e-d171f698239a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: twilio in /usr/local/lib/python3.10/dist-packages (9.3.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from twilio) (2.32.3)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from twilio) (2.9.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.4 in /usr/local/lib/python3.10/dist-packages (from twilio) (3.10.5)\n",
            "Requirement already satisfied: aiohttp-retry>=2.8.3 in /usr/local/lib/python3.10/dist-packages (from twilio) (2.8.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->twilio) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->twilio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->twilio) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->twilio) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->twilio) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp>=3.8.4->twilio) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "from twilio.rest import Client\n",
        "\n",
        "# Twilio account credentials (replace with your actual credentials)\n",
        "account_sid = 'ACd20952a40f90fa15fd6e06c0429552ae'\n",
        "auth_token = 'fd08d18ed9f6ea3d9dba22424f06594d'\n",
        "client = Client(account_sid, auth_token)\n",
        "\n",
        "# Function to send WhatsApp notification via Twilio\n",
        "def send_whatsapp_message(message_body):\n",
        "    message = client.messages.create(\n",
        "        from_='whatsapp:+14155238886',  # Twilio WhatsApp number\n",
        "        to='whatsapp:+966504625421',    # Recipient's WhatsApp number\n",
        "        body=message_body\n",
        "    )\n",
        "    print(f\"WhatsApp message sent with SID: {message.sid}\")\n",
        "\n",
        "# Load the custom YOLO model\n",
        "model = YOLO(\"/content/best.pt\")\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(\"/content/WhatsApp Video 2024-10-01 at 20.16.28_7047498d.mp4\")\n",
        "assert cap.isOpened(), \"Error reading video file\"\n",
        "\n",
        "# Get video properties\n",
        "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Video writer to save the output\n",
        "video_writer = cv2.VideoWriter(\"object_detection_output.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
        "\n",
        "# Object to detect and confidence threshold for sending notifications\n",
        "object_of_interest = \"accident\"  # Change to any class of interest\n",
        "confidence_threshold = 0.8  # Confidence level for sending notifications\n",
        "notification_sent = False  # To avoid duplicate notifications\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, im0 = cap.read()\n",
        "    if not success:\n",
        "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
        "        break\n",
        "\n",
        "    # Perform object detection\n",
        "    results = model.track(im0, persist=True, show=False)\n",
        "\n",
        "    # Filter detections by confidence\n",
        "    results[0].boxes = results[0].boxes[results[0].boxes.conf >= 0.6]\n",
        "\n",
        "    # Check for the object of interest in the detected objects\n",
        "    for box in results[0].boxes:\n",
        "        # Get the class label and confidence of the detected object\n",
        "        detected_class = model.names[int(box.cls)]\n",
        "        confidence = box.conf\n",
        "\n",
        "        # If the object of interest is detected with high confidence, send a WhatsApp message\n",
        "        if detected_class == object_of_interest and confidence >= confidence_threshold and not notification_sent:\n",
        "            message_body = f\"{object_of_interest.capitalize()} detected with {confidence * 100:.2f}% confidence.\"\n",
        "            send_whatsapp_message(message_body)\n",
        "            print(f\"WhatsApp notification sent for {object_of_interest}!\")\n",
        "            notification_sent = True  # Avoid sending multiple notifications for the same object\n",
        "\n",
        "    # Plot the results\n",
        "    im0_filtered = results[0].plot()\n",
        "\n",
        "    # Write the frame with detections to the output video\n",
        "    video_writer.write(im0_filtered)\n",
        "\n",
        "# Release the video capture and writer objects\n",
        "cap.release()\n",
        "video_writer.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(\"Video processing completed and saved to 'object_detection_output.mp4'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX1wiNri4U7k",
        "outputId": "bce722da-e7ae-41c8-f4d8-d7e117d5dda4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 320x640 1 moderate, 9.7ms\n",
            "Speed: 1.7ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.5ms\n",
            "Speed: 2.2ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 2.0ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.7ms\n",
            "Speed: 2.8ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.8ms\n",
            "Speed: 2.4ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 1.4ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 7.0ms\n",
            "Speed: 2.0ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.9ms\n",
            "Speed: 2.2ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.7ms\n",
            "Speed: 2.8ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.5ms\n",
            "Speed: 2.5ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.8ms\n",
            "Speed: 2.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 3.6ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 2.5ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 11.4ms\n",
            "Speed: 2.9ms preprocess, 11.4ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.8ms\n",
            "Speed: 2.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.5ms\n",
            "Speed: 2.0ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.3ms\n",
            "Speed: 2.9ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.4ms\n",
            "Speed: 1.9ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 8.8ms\n",
            "Speed: 2.2ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 9.6ms\n",
            "Speed: 3.6ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 1 moderate, 9.6ms\n",
            "Speed: 2.7ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 1 moderate, 7.0ms\n",
            "Speed: 1.8ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 1 moderate, 6.6ms\n",
            "Speed: 1.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 1 moderate, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.8ms\n",
            "Speed: 2.8ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.7ms\n",
            "Speed: 2.7ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 1.9ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.8ms\n",
            "Speed: 2.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.5ms\n",
            "Speed: 2.3ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.3ms\n",
            "Speed: 5.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.5ms\n",
            "Speed: 2.0ms preprocess, 6.5ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.7ms\n",
            "Speed: 1.5ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.5ms\n",
            "Speed: 2.7ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.0ms\n",
            "Speed: 2.8ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.8ms\n",
            "Speed: 2.7ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 3.2ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 1.9ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 2.6ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 13.2ms\n",
            "Speed: 3.3ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 2.5ms preprocess, 6.9ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.6ms\n",
            "Speed: 2.1ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.4ms\n",
            "Speed: 2.5ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.1ms\n",
            "Speed: 3.0ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.4ms\n",
            "Speed: 2.9ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 9.5ms\n",
            "Speed: 2.8ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 9.2ms\n",
            "Speed: 3.4ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 11.5ms\n",
            "Speed: 2.3ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.4ms\n",
            "Speed: 2.1ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.0ms\n",
            "Speed: 2.0ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.0ms\n",
            "Speed: 2.9ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.2ms\n",
            "Speed: 3.4ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.7ms\n",
            "Speed: 2.7ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.5ms\n",
            "Speed: 2.7ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.4ms\n",
            "Speed: 2.6ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.5ms\n",
            "Speed: 2.6ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.7ms\n",
            "Speed: 2.6ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.4ms\n",
            "Speed: 2.6ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 6.5ms\n",
            "Speed: 2.6ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.7ms\n",
            "Speed: 3.3ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.5ms\n",
            "Speed: 1.6ms preprocess, 6.5ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.5ms\n",
            "Speed: 2.0ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.9ms\n",
            "Speed: 2.9ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 0.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.4ms\n",
            "Speed: 2.3ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 2.2ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 2.1ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 2.2ms preprocess, 6.6ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 10.8ms\n",
            "Speed: 3.2ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 10.3ms\n",
            "Speed: 2.4ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 9.2ms\n",
            "Speed: 3.6ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 9.4ms\n",
            "Speed: 2.0ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.3ms\n",
            "Speed: 2.6ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.6ms\n",
            "Speed: 2.2ms preprocess, 10.6ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.9ms\n",
            "Speed: 3.6ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.0ms\n",
            "Speed: 2.9ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 2.0ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 2.1ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.9ms\n",
            "Speed: 2.0ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.7ms\n",
            "Speed: 2.8ms preprocess, 6.7ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.5ms\n",
            "Speed: 2.1ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.7ms\n",
            "Speed: 2.8ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 8.1ms\n",
            "Speed: 2.3ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 8.0ms\n",
            "Speed: 2.2ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.7ms\n",
            "Speed: 2.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 2.9ms preprocess, 6.9ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.8ms\n",
            "Speed: 2.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 (no detections), 7.1ms\n",
            "Speed: 2.1ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 1.7ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.9ms\n",
            "Speed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 10.3ms\n",
            "Speed: 3.3ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 9.6ms\n",
            "Speed: 2.2ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 9.6ms\n",
            "Speed: 2.6ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.3ms\n",
            "Speed: 2.1ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.5ms\n",
            "Speed: 2.1ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.2ms\n",
            "Speed: 2.2ms preprocess, 10.2ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 11.6ms\n",
            "Speed: 2.7ms preprocess, 11.6ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.4ms\n",
            "Speed: 2.5ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.8ms\n",
            "Speed: 2.6ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.8ms\n",
            "Speed: 2.1ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.7ms\n",
            "Speed: 1.5ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 2.8ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 2.8ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 2.3ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.8ms\n",
            "Speed: 1.5ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.8ms\n",
            "Speed: 2.8ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 2.7ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.8ms\n",
            "Speed: 2.8ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 14.6ms\n",
            "Speed: 4.3ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 12.4ms\n",
            "Speed: 2.2ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.2ms\n",
            "Speed: 3.8ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.9ms\n",
            "Speed: 1.9ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.5ms\n",
            "Speed: 3.5ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.1ms\n",
            "Speed: 1.8ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.0ms\n",
            "Speed: 2.1ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 3.0ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 2.7ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.2ms\n",
            "Speed: 2.9ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 2.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.3ms\n",
            "Speed: 3.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.2ms\n",
            "Speed: 2.1ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.4ms\n",
            "Speed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.6ms\n",
            "Speed: 2.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.4ms\n",
            "Speed: 2.2ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.1ms\n",
            "Speed: 2.9ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.9ms\n",
            "Speed: 2.1ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.3ms\n",
            "Speed: 3.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.9ms\n",
            "Speed: 2.3ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 6.8ms\n",
            "Speed: 1.7ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 7.7ms\n",
            "Speed: 3.0ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 12.7ms\n",
            "Speed: 2.4ms preprocess, 12.7ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 9.0ms\n",
            "Speed: 3.0ms preprocess, 9.0ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.1ms\n",
            "Speed: 3.6ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.0ms\n",
            "Speed: 2.3ms preprocess, 10.0ms inference, 3.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.6ms\n",
            "Speed: 2.3ms preprocess, 10.6ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.9ms\n",
            "Speed: 2.6ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 mild, 10.6ms\n",
            "Speed: 2.2ms preprocess, 10.6ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 2.6ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.7ms\n",
            "Speed: 2.9ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.6ms\n",
            "Speed: 3.0ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 8.0ms\n",
            "Speed: 3.5ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 7.0ms\n",
            "Speed: 3.3ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 10.5ms\n",
            "Speed: 2.2ms preprocess, 10.5ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.9ms\n",
            "Speed: 3.1ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.7ms\n",
            "Speed: 3.1ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 320x640 1 moderate, 6.8ms\n",
            "Speed: 2.9ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "Video frame is empty or video processing has been successfully completed.\n",
            "Video processing completed and saved to 'object_detection_output.mp4'.\n"
          ]
        }
      ]
    }
  ]
}